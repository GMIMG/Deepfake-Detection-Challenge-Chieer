{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "u5up_doZUl2D",
    "outputId": "11e5cfca-ff39-4eb8-e6e0-11f70e2e4735"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageChops, ImageEnhance, ImageDraw\n",
    "from mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BECDOb89Ul25"
   },
   "outputs": [],
   "source": [
    "test_sample = '../input/deepfake-detection-challenge/test_videos/'\n",
    "test_video_files = [test_sample + x for x in sorted(os.listdir(test_sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lZ0bQH3Ul3s"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    os.makedirs('../working/dataset/TEST3')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pM7gXgzZUl3x"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZKag2e7hUl33"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'original_vidio_has_fake' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-0bbe106c4a76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_video_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_vidio_has_fake\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_video_files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'original_vidio_has_fake' is not defined"
     ]
    }
   ],
   "source": [
    "train_video_files = sorted(original_vidio_has_fake)\n",
    "train_video_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 892
    },
    "colab_type": "code",
    "id": "eNk7dmOOUl38",
    "outputId": "a6a3049f-b02a-41f7-a824-ff3cc1c5219e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/42\n",
      "1/42\n",
      "2/42\n",
      "3/42\n",
      "4/42\n",
      "5/42\n",
      "6/42\n",
      "7/42\n",
      "8/42\n",
      "9/42\n",
      "10/42\n",
      "11/42\n",
      "12/42\n",
      "13/42\n",
      "14/42\n",
      "15/42\n",
      "16/42\n",
      "17/42\n",
      "18/42\n",
      "19/42\n",
      "20/42\n",
      "21/42\n",
      "22/42\n",
      "23/42\n",
      "24/42\n",
      "25/42\n",
      "26/42\n",
      "27/42\n",
      "28/42\n",
      "29/42\n",
      "30/42\n",
      "31/42\n",
      "32/42\n",
      "33/42\n",
      "34/42\n",
      "35/42\n",
      "36/42\n",
      "37/42\n",
      "38/42\n",
      "39/42\n",
      "40/42\n",
      "41/42\n"
     ]
    }
   ],
   "source": [
    "# origin 영상들로 학습시작\n",
    "#detector = dlib.get_frontal_face_detector()\n",
    "train_video_files = sorted(original_vidio_has_fake)\n",
    "\n",
    "#train_video_files = ['atvmxvwyns.mp4']\n",
    "\n",
    "mtcnn = MTCNN() # device='cuda:0',\n",
    "\n",
    "vid_num = 0\n",
    "all_vid = len(train_video_files)\n",
    "# origin 영상 얼굴찾기\n",
    "for vid in train_video_files:\n",
    "    count = 0\n",
    "    path = '../input/deepfake-detection-challenge/train_sample_videos/' + vid\n",
    "    file_name = vid.split('.')[0]\n",
    "    face_coord = {}\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frame = 0\n",
    "\n",
    "    \n",
    "    before_face_img_coord = []\n",
    "    \n",
    "    while(cap.isOpened()):\n",
    "        ret = cap.grab()\n",
    "        if ret == False:\n",
    "            break\n",
    "        #if frame % 30 == 0:\n",
    "        ret, image = cap.retrieve()\n",
    "        #img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img = image\n",
    "        face_coord[frame]=[]\n",
    "        # 처음 얼굴이 나오면\n",
    "        if not before_face_img_coord:\n",
    "            faces = mtcnn.detect_faces(img)\n",
    "            # 얼굴 한개 이상?\n",
    "            for face_idx, face in enumerate(faces):\n",
    "                if 'confidence' in face and face['confidence'] > 0.9:\n",
    "                    x1,y1,w,h = face['box']\n",
    "                    x2 = min(x1+w, img.shape[1])\n",
    "                    y2 = min(y1+h, img.shape[0])\n",
    "                    x1 = max(x1, 0)\n",
    "                    y1 = max(y1, 0)\n",
    "                    crop_img = img[y1:y2, x1:x2]\n",
    "                    face_coord[frame].append([y1,y2,x1,x2])\n",
    "\n",
    "#                     plt.imshow(crop_img)\n",
    "#                     plt.show()\n",
    "#                     print(frame)\n",
    "                    cv2.imwrite(f'../working/dataset/REAL3/REAL_{file_name}_{frame}_{face_idx}.jpg', cv2.resize(crop_img, (160, 160)))\n",
    "                    before_face_img_coord.append([y1,y2,x1,x2])\n",
    "                    \n",
    "                    break\n",
    "                    \n",
    "        # 이전에 검출된 face가 있으면\n",
    "        else:\n",
    "            for face_idx, coord in enumerate(before_face_img_coord):\n",
    "                im = img[max(coord[0]-50,0) : min(coord[1]+50,img.shape[0]),\n",
    "                         max(coord[2]-50,0) : min(coord[3]+50,img.shape[1])]\n",
    "                face = mtcnn.detect_faces(im)\n",
    "                if face:\n",
    "                    # 1개만 저장해\n",
    "                    x11,y11,w,h = face[0]['box']\n",
    "                    x1 = max(max(coord[2]-50, 0) + x11, 0)\n",
    "                    x2 = min(x1 + w, img.shape[1])\n",
    "                    y1 = max(max(coord[0]-50, 0) + y11, 0)\n",
    "                    y2 = min(y1 + h, img.shape[0])\n",
    "                    crop_img2 = img[y1:y2, x1:x2]\n",
    "                    \n",
    "                    \n",
    "                    face_coord[frame].append([y1,y2,x1,x2])\n",
    "                    before_face_img_coord[face_idx]=[y1,y2,x1,x2]\n",
    "                    \n",
    "#                     plt.imshow(crop_img2)\n",
    "#                     plt.show()\n",
    "#                     print(frame)\n",
    "                    cv2.imwrite(f'../working/dataset/REAL3/REAL_{file_name}_{frame}_{face_idx}.jpg', cv2.resize(crop_img2, (160, 160)))\n",
    "                    crop_img = crop_img2\n",
    "                else:\n",
    "                    #face_coord[frame].append([y1,y2,x1,x2])\n",
    "                    cv2.imwrite(f'../working/dataset/REAL3/REAL_{file_name}_{frame}_{face_idx}.jpg', cv2.resize(crop_img, (160, 160)))\n",
    "\n",
    "        frame += 1\n",
    "        if not face_coord[0]:\n",
    "            break\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "    \n",
    "    # 이비디오의 fake 영상만큼 반복\n",
    "    for fake_vid in train_sample_metadata[train_sample_metadata['original']==vid].index:\n",
    "        if not face_coord[0]:\n",
    "            break\n",
    "        fake_path = '../input/deepfake-detection-challenge/train_sample_videos/' + fake_vid\n",
    "        cap2 = cv2.VideoCapture(fake_path)\n",
    "        fake_vid_name = fake_vid.split('.')[0]\n",
    "        frame2 = 0\n",
    "        while(cap2.isOpened()):\n",
    "            ret2 = cap2.grab()\n",
    "            if ret2 == False:\n",
    "                break\n",
    "            #if frame2 % 30 == 0:\n",
    "            ret2, image2 = cap2.retrieve()\n",
    "            #img2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "            if face_coord[frame2]:\n",
    "                for face_idx, i in enumerate(face_coord[frame2]):\n",
    "                    y1,y2,x1,x2 = i\n",
    "                    crop_img3 = image2[y1:y2, x1:x2]\n",
    "                    cv2.imwrite(f'../working/dataset/FAKE3/FAKE_{fake_vid_name}_{frame2}_{face_idx}_{file_name}.jpg', cv2.resize(crop_img3, (160, 160)))\n",
    "            else:\n",
    "#                 print(fake_vid, frame2)\n",
    "#                 plt.imshow(crop_img3)\n",
    "#                 plt.show()\n",
    "                cv2.imwrite(f'../working/dataset/FAKE3/FAKE_{fake_vid_name}_{frame2}_{face_idx}_{file_name}.jpg', cv2.resize(crop_img3, (160, 160)))\n",
    "            frame2 += 1\n",
    "        cap2.release()\n",
    "\n",
    "        \n",
    "    print(f'{vid_num}/{all_vid}')\n",
    "    vid_num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "1_capture_train_img_link_origin_fake-MTCNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
